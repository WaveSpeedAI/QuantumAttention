# QuantumAttention

Better FP8 attention for Hopper

Forked from [flash-attention](https://github.com/Dao-AILab/flash-attention/tree/6b1d059eda21c1bd421f3d352786fca2cab61954)
